{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "##calculate the distance square\n",
    "def dist_sq(a, b, axis = 0):\n",
    "    '''a,b are numpy arrays with the same shape'''\n",
    "    return np.sum((a-b)**2,axis)\n",
    "\n",
    "\n",
    "##calculate the cost\n",
    "def cost(data,centroid):\n",
    "    '''The function caculates the sum of the distance square of each point in the data set to its closest centroid\n",
    "    parameters: data and centroids\n",
    "    output:cost(a number)\n",
    "    '''\n",
    "    return np.sum([min(dist_sq(centroid, d, axis = 1)) for d in data])\n",
    "\n",
    "##calculate the parallel probability\n",
    "def dist_prob_parallel(data,centroid,l):\n",
    "    '''The function caculates the sum of the distance square to the closest centroid for each point in the data set\n",
    "    parameters: data and centroids\n",
    "    output:probability with length=len(data)\n",
    "    '''\n",
    "    phi= cost(data,centroid)\n",
    "    return np.array([(min(dist_sq(centroid, d, axis = 1)))*l/phi for d in data])\n",
    "\n",
    "\n",
    "#weight probability\n",
    "def weight_prob(data,centroid):\n",
    "    closest_center = [np.argmin(dist_sq(centroid, d, axis = 1)) for d in data]\n",
    "    ## number of points which is closest to each s in c\n",
    "    number_= np.array([closest_center.count(i) for i in range(len(centroid))]).astype(float)\n",
    "    ## return normalized weight\n",
    "    return number_/np.sum(number_)\n",
    "\n",
    "\n",
    "#step 5: recluster the weighted points in C into k clusters\n",
    "#reinitialize k centroids\n",
    "def reassign_centroids(data,centroid,k,l,w):\n",
    "    new_centroids = data[np.random.choice(range(len(centroid)),size=1,p=w),]\n",
    "    potential_centroids = centroid\n",
    "    for i in range(k-1):\n",
    "        prob = dist_prob_parallel(potential_centroids,new_centroids,l) * w\n",
    "        new_centroid = data[np.random.choice(range(len(centroid)),size=1,p=prob/np.sum(prob)),]\n",
    "        new_centroids = np.vstack((new_centroids,new_centroid))\n",
    "    return new_centroids\n",
    "\n",
    "    \n",
    "\n",
    "def kmeansparallel(data, k, l, r):\n",
    "    #step 1: sample a point uniformly at random from X\n",
    "    centroid=np.array(data[np.random.choice(range(len(data)),1),])\n",
    "    \n",
    "    #step 2: calculate number of iteration\n",
    "    iteration= np.ceil(np.log(cost(data,centroid))).astype(int)  \n",
    "    \n",
    "    #step 3: Get initial Centroids C\n",
    "    for round in range(r):\n",
    "        for i in range(iteration):\n",
    "            centroid_added = data[dist_prob_parallel(data,centroid,l)>np.random.uniform(size = len(data)),]\n",
    "            centroid = np.vstack((centroid,centroid_added))  \n",
    "    \n",
    "    #step 4: calculate the weight probability\n",
    "    w=weight_prob(data,centroid)\n",
    "    \n",
    "    #step 5: recluster the weighted points in C into k clusters\n",
    "    #reinitialize k centroids\n",
    "    final_centroids=reassign_centroids(data,centroid,k,l,w)\n",
    "    \n",
    "    return final_centroids\n",
    "\n",
    "    \n",
    "\n",
    "#with the initialization of the centroids from the function kmeansplusplus\n",
    "#plug in the original data(dataSet), initializtions(initial) and the dimension of the data(d)\n",
    "def kmeans(dataSet, initial, k, d):\n",
    "    centroids=initial\n",
    "    # Initialize book keeping vars.\n",
    "    iterations = 0\n",
    "    oldCentroids = np.zeros(initial.shape)\n",
    "    \n",
    "    # Run the main k-means algorithm\n",
    "    while not shouldStop(oldCentroids, centroids, iterations):\n",
    "        # Save old centroids for convergence test. Book keeping.\n",
    "        oldCentroids = centroids\n",
    "        iterations += 1\n",
    "        \n",
    "        # Assign labels to each datapoint based on centroids\n",
    "        l= getLabels(dataSet, centroids)\n",
    "        \n",
    "        # Assign centroids based on datapoint labels\n",
    "        centroids = getCentroids(dataSet, l, k, d)\n",
    "        \n",
    "    # We can get the labels too by calling getLabels(dataSet, centroids)\n",
    "    return centroids, np.array(l)\n",
    "# Function: Should Stop\n",
    "# -------------\n",
    "# Returns True or False if k-means is done. K-means terminates either\n",
    "# because it has run a maximum number of iterations OR the centroids\n",
    "# stop changing.\n",
    "def shouldStop(oldCentroids, centroids, iterations):\n",
    "    if iterations > 50: return True\n",
    "    return oldCentroids.all == centroids.all\n",
    "# Function: Get Labels\n",
    "# -------------\n",
    "# Returns a label for each piece of data in the dataset. \n",
    "def getLabels(dataSet, centroids):\n",
    "    # For each element in the dataset, chose the closest centroid. \n",
    "    # Make that centroid the element's label.\n",
    "    l=[]\n",
    "    for i in range(dataSet.shape[0]):\n",
    "        #arg min as the label\n",
    "        l.append(np.argmin(list(dist_sq(dataSet[i],c) for c in centroids)))\n",
    "    return l\n",
    "# Function: Get Centroids\n",
    "# -------------\n",
    "# Returns k random centroids, each of dimension n.\n",
    "def getCentroids(dataSet, labels, k, d):\n",
    "    # Each centroid is the arithmetic mean of the points that\n",
    "    # have that centroid's label. Important: If a centroid is empty (no points have\n",
    "    data_new = DataFrame(dataSet.copy())\n",
    "    data_new['Labels'] = labels\n",
    "    data_new = np.array(data_new.groupby(['Labels']).mean().iloc[:,:d])\n",
    "  \n",
    "    return data_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import simulatedData\n",
    "from simulatedData import generate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_data=generate_data(4,50,2)\n",
    "k=3\n",
    "l=2*k\n",
    "d=2\n",
    "r=1\n",
    "centroid=sim_data[np.random.choice(range(100),k),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c=kmeansparallel(sim_data, k, l, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 12.4740532 ,  13.59686713],\n",
       "        [  4.19262668,   0.87382565],\n",
       "        [ 13.55633378,  12.42110573]]), array([1, 1, 0, ..., 1, 1, 1]))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans(sim_data,c,k,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
