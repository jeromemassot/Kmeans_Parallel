{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "\n",
    "#distance square\n",
    "def dist_sq(a, b):\n",
    "    return np.sum((a-b)**2)\n",
    "#minimum distance square for every point to the centroid\n",
    "def point_sq(data,centroid):\n",
    "    dist=[]\n",
    "    for i in range(data.shape[0]):\n",
    "        dist.append(min(dist_sq(data[i],c) for c in centroid))\n",
    "    return dist\n",
    "        \n",
    "#calculate probability\n",
    "def dist_prob_plus(Dist):\n",
    "    return Dist/np.sum(Dist)\n",
    "\n",
    "#calculate probability\n",
    "def dist_prob_parallel(Dist,l):\n",
    "    return l*Dist/np.sum(Dist)\n",
    "\n",
    "\n",
    "#step 2: calculate the cost and number of iterations(log(cost))\n",
    "def log_cost(data_copy,centroid):\n",
    "    cost=np.sum(point_sq(data_copy,centroid))\n",
    "    iteration=math.ceil(np.log(cost))\n",
    "    return iteration\n",
    "\n",
    "    \n",
    "    \n",
    "#calculate weights\n",
    "#step 4: assign the weights\n",
    "def weight_prob(data_copy, centroid):\n",
    "    w_size=centroid.shape[0]\n",
    "    w=np.zeros(w_size)\n",
    "    for i in range(data_copy.shape[0]):\n",
    "        index_w=np.argmin(list(dist_sq(data_copy[i],c) for c in centroid))\n",
    "        w[index_w]=w[index_w]+1\n",
    "    return w\n",
    "\n",
    "\n",
    "\n",
    "#step 5: recluster the weighted points in C into k clusters\n",
    "#reinitialize k centroids\n",
    "def reassign_centroids(centroid,k,d,w):\n",
    "    new_centroid=np.zeros([k,d])\n",
    "    for cluster in range(k):\n",
    "        #according to the weights from step 4, calculate the probability that a point is sampled from C\n",
    "        prob_w=list(w/sum(w))\n",
    "        #sample a new centroid\n",
    "        new_index=np.random.choice(centroid.shape[0],1,prob_w)\n",
    "        #store the new centroid\n",
    "        new_centroid[cluster]=centroid[new_index]\n",
    "        #delete the new centroid from the centroid\n",
    "        centroid=np.delete(centroid,new_index,axis=0)\n",
    "        #delete the correponding weight\n",
    "        w=np.delete(w,new_index,axis=0)\n",
    "    return new_centroid\n",
    "\n",
    "\n",
    "def kmeansplusplus(data, k, d):\n",
    "    #make a copy of the data\n",
    "    data_copy=data.copy()\n",
    "    #step 1: sample a point uniformly at random from x\n",
    "    index=int(np.random.choice(data_copy.shape[0],1))\n",
    "    centroid=data_copy[index]\n",
    "    #once the centroid is determined, delete it from the copy \n",
    "    data_copy=np.delete(data_copy,index,axis=0)\n",
    "    #step 2: while c<k, sample x from X with probability d^2/phi_x(C)\n",
    "    for number in range(k-1):\n",
    "        #calculate the square difference for every point in the copy to its nearest center\n",
    "        distance=point_sq(data_copy,centroid)\n",
    "        #calculate the probability\n",
    "        prob=dist_prob_plus(distance).tolist()\n",
    "        #randomly sample another centroid\n",
    "        index=int(np.random.choice(data_copy.shape[0],1,prob))\n",
    "        #add the new centroid\n",
    "        centroid=np.vstack([centroid,data_copy[index]])\n",
    "        #delete the new centroid from the copy\n",
    "        data_copy=np.delete(data_copy,index,axis=0)\n",
    "    return centroid\n",
    "\n",
    "\n",
    "\n",
    "def kmeansparallel(data, k, l, d):\n",
    "    #step 1: sample a point uniformly at random from X\n",
    "    index=int(np.random.choice(data.shape[0],1))\n",
    "    centroid=np.array(data[index])\n",
    "    data_copy=data.copy()\n",
    "    data_copy=np.delete(data_copy,index,axis=0)\n",
    "    \n",
    "    #step 2: calculate number of iteration\n",
    "    iteration= log_cost(data_copy,centroid)\n",
    "    \n",
    "    #step 3: Get initial Centroids C\n",
    "    for number in range(iteration):\n",
    "        #calculate phi_X(C)\n",
    "        distance=point_sq(data_copy,centroid)\n",
    "        #calculate the probability\n",
    "        prob=dist_prob_parallel(distance,l).tolist()\n",
    "        for n in range(data_copy.shape[0]):\n",
    "            #if the probability is greater than the random uniform\n",
    "            if prob[n]>np.random.uniform():\n",
    "                #add the point to C\n",
    "                centroid=np.vstack([centroid,np.array(data_copy[n])])\n",
    "                #delete that point from the copy\n",
    "                data_copy=np.delete(data_copy,n,axis=0)\n",
    "    \n",
    "    #step 4: calculate the weight probability\n",
    "    w=weight_prob(data_copy,centroid)\n",
    "    \n",
    "    #step 5: recluster the weighted points in C into k clusters\n",
    "    #reinitialize k centroids\n",
    "    new_centroids=reassign_centroids(centroid,k,d,w)\n",
    "    \n",
    "    return new_centroids\n",
    "\n",
    "    \n",
    "#with the initialization of the centroids from the function kmeansplusplus\n",
    "#plug in the original data(dataSet), initializtions(initial) and the dimension of the data(d)\n",
    "def kmeans(dataSet, initial, k, d):\n",
    "    centroids=initial\n",
    "    # Initialize book keeping vars.\n",
    "    iterations = 0\n",
    "    oldCentroids = np.zeros(initial.shape)\n",
    "    \n",
    "    # Run the main k-means algorithm\n",
    "    while not shouldStop(oldCentroids, centroids, iterations):\n",
    "        # Save old centroids for convergence test. Book keeping.\n",
    "        oldCentroids = centroids\n",
    "        iterations += 1\n",
    "        \n",
    "        # Assign labels to each datapoint based on centroids\n",
    "        l= getLabels(dataSet, centroids)\n",
    "        \n",
    "        # Assign centroids based on datapoint labels\n",
    "        centroids = getCentroids(dataSet, l, k, d)\n",
    "        \n",
    "    # We can get the labels too by calling getLabels(dataSet, centroids)\n",
    "    return centroids, np.array(l)\n",
    "# Function: Should Stop\n",
    "# -------------\n",
    "# Returns True or False if k-means is done. K-means terminates either\n",
    "# because it has run a maximum number of iterations OR the centroids\n",
    "# stop changing.\n",
    "def shouldStop(oldCentroids, centroids, iterations):\n",
    "    if iterations > 50: return True\n",
    "    return oldCentroids.all == centroids.all\n",
    "# Function: Get Labels\n",
    "# -------------\n",
    "# Returns a label for each piece of data in the dataset. \n",
    "def getLabels(dataSet, centroids):\n",
    "    # For each element in the dataset, chose the closest centroid. \n",
    "    # Make that centroid the element's label.\n",
    "    l=[]\n",
    "    for i in range(dataSet.shape[0]):\n",
    "        #arg min as the label\n",
    "        l.append(np.argmin(list(dist_sq(dataSet[i],c) for c in centroids)))\n",
    "    return l\n",
    "# Function: Get Centroids\n",
    "# -------------\n",
    "# Returns k random centroids, each of dimension n.\n",
    "def getCentroids(dataSet, labels, k, d):\n",
    "    # Each centroid is the arithmetic mean of the points that\n",
    "    # have that centroid's label. Important: If a centroid is empty (no points have\n",
    "    # that centroid's label) you should randomly re-initialize it.\n",
    "    data_new = DataFrame(dataSet.copy())\n",
    "    data_new['Labels'] = labels\n",
    "    data_new = np.array(data_new.groupby(['Labels']).mean().iloc[:,:d])\n",
    "    # if a centroid is empty, reinitialize it \n",
    "    if len(np.unique(labels))<k:\n",
    "        diff=k-len(np.unique(labels))\n",
    "        data_new=np.vstack([data_new,np.random.random([diff,d])])    \n",
    "    return data_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
