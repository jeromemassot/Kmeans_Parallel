{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n"
     ]
    }
   ],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "\n",
    "#distance square\n",
    "def dist_sq(a, b):\n",
    "    return np.sum((a-b)**2)\n",
    "\n",
    "\n",
    "#minimum distance square for every point to the centroid\n",
    "def point_sq(data,centroid):\n",
    "    dist=[min(dist_sq(d,c) for c in centroid) for d in data]\n",
    "    return dist\n",
    "\n",
    "#calculate probability\n",
    "def dist_prob_parallel(Dist,l):\n",
    "    return l*Dist/np.sum(Dist)\n",
    "\n",
    "\n",
    "#step 2: calculate the cost and number of iterations(log(cost))\n",
    "def log_cost(data_copy,centroid):\n",
    "    cost=np.sum(point_sq(data_copy,centroid))\n",
    "    iteration=math.ceil(np.log(cost))\n",
    "    return iteration\n",
    "\n",
    "    \n",
    "    \n",
    "#calculate weights\n",
    "#step 4: assign the weights\n",
    "#calculate weights\n",
    "#step 4: assign the weights\n",
    "def weight_prob(data_copy, centroid):\n",
    "    weight=[np.argmin(list(dist_sq(d,c) for c in centroid)) for d in data_copy]\n",
    "    w=np.array([weight.count(i) for i in range(len(data_copy))])\n",
    "    return w\n",
    "\n",
    "\n",
    "\n",
    "#step 5: recluster the weighted points in C into k clusters\n",
    "#reinitialize k centroids\n",
    "def reassign_centroids(centroid,k,d,w):\n",
    "    new_centroid=np.zeros([k,d])\n",
    "    for cluster in range(k):\n",
    "        #according to the weights from step 4, calculate the probability that a point is sampled from C\n",
    "        prob_w=list(w/sum(w))\n",
    "        #sample a new centroid\n",
    "        new_index=np.random.choice(centroid.shape[0],1,prob_w)\n",
    "        #store the new centroid\n",
    "        new_centroid[cluster]=centroid[new_index]\n",
    "        #delete the new centroid from the centroid\n",
    "        centroid=np.delete(centroid,new_index,axis=0)\n",
    "        #delete the correponding weight\n",
    "        w=np.delete(w,new_index,axis=0)\n",
    "    return new_centroid\n",
    "\n",
    "\n",
    "\n",
    "def kmeansparallel(data, k, l, d, r):\n",
    "    #step 1: sample a point uniformly at random from X\n",
    "    index=int(np.random.choice(data.shape[0],1))\n",
    "    centroid=np.array(data[index])\n",
    "    data_copy=data.copy()\n",
    "    data_copy=np.delete(data_copy,index,axis=0)\n",
    "    \n",
    "    #step 2: calculate number of iteration\n",
    "    iteration= log_cost(data_copy,centroid)\n",
    "    \n",
    "    #step 3: Get initial Centroids C\n",
    "    for round in range(r):\n",
    "        for number in range(iteration):\n",
    "        #calculate phi_X(C)\n",
    "            distance=point_sq(data_copy,centroid)\n",
    "        #calculate the probability\n",
    "            prob=dist_prob_parallel(distance,l).tolist()\n",
    "            for n in range(data_copy.shape[0]):\n",
    "            #if the probability is greater than the random uniform\n",
    "                if prob[n]>np.random.uniform():\n",
    "                #add the point to C\n",
    "                    centroid=np.vstack([centroid,np.array(data_copy[n])])\n",
    "                #delete that point from the copy\n",
    "                    data_copy=np.delete(data_copy,n,axis=0)\n",
    "    \n",
    "    #step 4: calculate the weight probability\n",
    "    w=weight_prob(data_copy,centroid)\n",
    "    \n",
    "    #step 5: recluster the weighted points in C into k clusters\n",
    "    #reinitialize k centroids\n",
    "    new_centroids=reassign_centroids(centroid,k,d,w)\n",
    "    \n",
    "    return new_centroids\n",
    "\n",
    "    \n",
    "#with the initialization of the centroids from the function kmeansplusplus\n",
    "#plug in the original data(dataSet), initializtions(initial) and the dimension of the data(d)\n",
    "def kmeans(dataSet, initial, k, d):\n",
    "    centroids=initial\n",
    "    # Initialize book keeping vars.\n",
    "    iterations = 0\n",
    "    oldCentroids = np.zeros(initial.shape)\n",
    "    \n",
    "    # Run the main k-means algorithm\n",
    "    while not shouldStop(oldCentroids, centroids, iterations):\n",
    "        # Save old centroids for convergence test. Book keeping.\n",
    "        oldCentroids = centroids\n",
    "        iterations += 1\n",
    "        \n",
    "        # Assign labels to each datapoint based on centroids\n",
    "        l= getLabels(dataSet, centroids)\n",
    "        \n",
    "        # Assign centroids based on datapoint labels\n",
    "        centroids = getCentroids(dataSet, l, k, d)\n",
    "        \n",
    "    # We can get the labels too by calling getLabels(dataSet, centroids)\n",
    "    return centroids, np.array(l)\n",
    "# Function: Should Stop\n",
    "# -------------\n",
    "# Returns True or False if k-means is done. K-means terminates either\n",
    "# because it has run a maximum number of iterations OR the centroids\n",
    "# stop changing.\n",
    "def shouldStop(oldCentroids, centroids, iterations):\n",
    "    if iterations > 50: return True\n",
    "    return oldCentroids.all == centroids.all\n",
    "# Function: Get Labels\n",
    "# -------------\n",
    "# Returns a label for each piece of data in the dataset. \n",
    "def getLabels(dataSet, centroids):\n",
    "    # For each element in the dataset, chose the closest centroid. \n",
    "    # Make that centroid the element's label.\n",
    "    \n",
    "    l=[np.argmin(list(dist_sq(d,c) for c in centroids)) for d in dataSet]\n",
    "    return l\n",
    "# Function: Get Centroids\n",
    "# -------------\n",
    "# Returns k random centroids, each of dimension n.\n",
    "def getCentroids(dataSet, labels, k, d):\n",
    "    # Each centroid is the arithmetic mean of the points that\n",
    "    # have that centroid's label.\n",
    "    data_new = DataFrame(dataSet.copy())\n",
    "    data_new['Labels'] = labels\n",
    "    data_new = np.array(data_new.groupby(['Labels']).mean().iloc[:,:d])\n",
    "  \n",
    "    return data_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import simulatedData\n",
    "from simulatedData import generate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_data_k6 = generate_data(k=6, var=100, dim=15, npoints=10000)\n",
    "k = 6\n",
    "l=k*2\n",
    "d = 15\n",
    "r = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_cython=kmeansparallel(sim_data_k6, k, l, d, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 22.9 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit kmeansparallel(sim_data_k6, k, l, d, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit kmeans(sim_data_k6, c_cython, k, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_kmeans_cython=kmeans(sim_data_k6,c_cython,k,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_cython = c_kmeans_cython[0][:,0]\n",
    "y_cython = c_kmeans_cython[0][:,1]\n",
    "labels_cython = c_kmeans_cython[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(sim_data_k6[:,0], sim_data_k6[:,1], c=labels_cython, cmap='Accent')\n",
    "plt.scatter(x_cython, y_cython, s=40, marker=\"o\", c=\"darkblue\")\n",
    "plt.title(\"K-means++ clustering\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
